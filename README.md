# LLM Life cycle
This repository serves as a comprehensive resource for understanding and managing the entire lifecycle of Large Language Models (LLMs). It covers various aspects, including:

* LLM Training: Learn how to train your own LLM from scratch or fine-tune existing models for specific tasks.
* Deployment: Explore different techniques to deploy your LLM for efficient inference and real-world applications.
* LLM-based Applications: Discover various use cases and practical applications for LLMs across various domains.
* GenAI Fundamentals: Gain insights into the underlying concepts and technologies that power generative AI models.
Start your journey into the fascinating world of LLMs with this repository!

# [LLM Deployment]("LLM_Deployment")
There are different ways to deploy LLMs and serve the model for Inference.
## Local Deployment:
* Text-Generation-Inference (TGI)
* Using FastAPI
